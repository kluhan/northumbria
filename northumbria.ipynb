{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Classify Song Lyrics according to Genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: \n",
    "* Andreas Bonny\n",
    "* Niclas Lensch\n",
    "* Klaus Luhan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to find out whether it is possible to predict the genre of a song based on its lyrics. To\n",
    "this end, devise one or multiple approaches based on the methods discussed in the lecture and evaluate how\n",
    "well they perform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we analyse lyrics which are written in human language, this work makes heavy use of the Natural Language Toolkit, the leading platform to work with human language Data. To use NLTK the needed ressource files have to be downloaded by using the\n",
    ">nltk.download()\n",
    "\n",
    "command. It enables us to easily use data analyzing methods such as tokenization or stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This section can be ignored if there is no valid GPU set up*\n",
    "\n",
    "Because the Project is performance heavy, we recommend the usage of GPU and CPU to improve , if there is a GPU available but not set up, we recommend adding it to your python-environment. Furthermore to use the GPU the Cuda Framework needs to be installed. It is very important to watch out the Version of cuda it **needs** to match with the tensorflow version installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU')\n",
    "\n",
    "print(\"There is\", len(gpus), \"detected GPU\")\n",
    "print(\"There is\", len(cpus), \"detected CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally it is recommended to enable dynamic memory growth for the GPU. If not available the GPU will reserve all video memory, which can lead to System instabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@starriet87/tensorflow-2-0-wanna-limit-gpu-memory-10ad474e2528\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the data we need to prepare and preprocess it. After that the subset of the original dataset can be used in neural networks and the like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the two csv files *artists-data.csv* and *lyrics-data.csv*  are loaded into the corresponding pandas dataframes as shown below. Thereafter the dataframes are joined together and saved into  raw_df to get one dataframe with the entire required data, which can be seen in the following code segment. Additionally the identifiers are transformed into lowercase, to enable a convenient use. After all this is done the dataframe raw_df contains {{ len(raw_df)}} loaded datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load form Disk and join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "song_df = pd.read_csv('./song-lyrics/lyrics-data.csv')\n",
    "author_df = pd.read_csv('./song-lyrics/artists-data.csv')\n",
    "\n",
    "song_df = song_df.rename(columns={'ALink': 'Link'})\n",
    "\n",
    "raw_df = song_df.merge(author_df, on='Link') \n",
    "raw_df.columns = raw_df.columns.str.lower()\n",
    "\n",
    "print(\"There are\", len(raw_df), \"datasets loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unneeded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our ~227000 datasets are loaded the unneeded Data must be removed to ensure a better performance and more accurate results.\n",
    "First every column which is not needed to categorize the lyrics to a genre is dropped.\n",
    "Thereafter every lyric which is not in English gets removed, since the task is to only categorize english songs.\n",
    "At last every duplicate song is removed to ensure a clear dataset.\n",
    "These two steps reduce the data to {{Test}} datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns_to_drop = ['sname','slink','link', 'popularity', 'genres', 'songs', 'link', 'idiom']\n",
    "\n",
    "filterd_df = raw_df[raw_df.idiom.eq('ENGLISH')]\n",
    "filterd_df = filterd_df.drop_duplicates(subset='slink', keep='first') \n",
    "filterd_df = filterd_df.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the cleanup of the overall data, the lyrics are now further processed. First of all, before the more complex processings, all lyrics are transformed into lower case to enable a consistent use. After that the two techniques tokenization and lemmatization are used in the general preprocessing. Each of them using NLTK and separated into its own code block. Furthermore after these two methods are done, the data is cleaned up to, agaian, enable a convenient use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "transformed_df = filterd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "transformed_df['lyric'] = transformed_df['lyric'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first big step in the preprocessing is the tokenization, which consists of two parts. The first part is transforming every lyric into verses. The second part transforms the resulting verses into word-level-tokens. The result of the tokenization is a datastream with a list of verses, which in turn contains a list of words. This possibly enables more variation of data that can be used in the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.parsing.preprocessing import STOPWORDS, remove_stopwords\n",
    "\n",
    "def lyric_to_verse(lyric):\n",
    "    return nltk.sent_tokenize(lyric,language='english')\n",
    "\n",
    "def verse_to_word(verse):\n",
    "    customStopwords = [\"'s'\", \"n't\", \"aint\", \"oh\", \"im\", \"yeah\", \"youre\", \"cant\", \"dont\", \"em\", \"you'll\", \"it's\", \"hey\"]\n",
    "    all_stopwords = STOPWORDS.union(set(customStopwords))\n",
    "    \n",
    "    verse = remove_stopwords(verse)\n",
    "    return nltk.word_tokenize(verse, language='english')\n",
    "\n",
    "print(\"Pre: \", transformed_df['lyric'])\n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: lyric_to_verse(x)) \n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: [verse_to_word(verse) for verse in x])\n",
    "print(\"Post: \", transformed_df['lyric'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method of preprocess is the lemmatization. The process consists of stemming and then the actual lemmatization. The stemming leads back a single word to its word stem. Thereafter the stemmed word gets categorized into the word class it has. To simplify this, since a completely detailed lemmatization is not needed, the word classes get mapped to the basic classes of adjective, noun, adverb and verb. After this the data is returned and the only step left is to clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "#by http://textmining.wp.hs-hannover.de/Preprocessing.html\n",
    "def pos_tagger(sentences):\n",
    "    return [nltk.pos_tag(sent) for sent in sentences]\n",
    "\n",
    "def wntag(pttag):\n",
    "    if pttag in ['JJ', 'JJR', 'JJS']:\n",
    "        return wordnet.ADJ\n",
    "    elif pttag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        return wordnet.NOUN\n",
    "    elif pttag in ['RB', 'RBR', 'RBS']:\n",
    "        return wordnet.ADV\n",
    "    elif pttag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        return wordnet.VERB\n",
    "    return None\n",
    "\n",
    "def lemmatize(lemmatizer,word,pos):\n",
    "    if pos == None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word,pos)\n",
    "    \n",
    "def lem(sentences):\n",
    "    transformed = []\n",
    "    for sentence in sentences:\n",
    "        transformed.append([lemmatize(lemmatizer,word,wntag(pos)) for (word,pos) in sentence])\n",
    "    return transformed \n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: pos_tagger(x)) \n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: lem(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last part of the lemmatization the resulting data of the first two segments are cleaned up. We remove any artifacts that survived the lemmatization such as empty words, single letter words and all non alphabetic characters. The second part of the Cleanup is to remove stopwords. To do that gensim is used in order to gain a preexisting list while also adding a few other stopwords. As a result of this partial step, we gain an object that contains the lyrics without the found stopwords in vers-level-tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_non_alphabetic_characters(sentences):\n",
    "    cleaned = []\n",
    "    for sentence in sentences:\n",
    "        cleaned.append(list(map(lambda x: re.sub(\"[\\W_]\", '', x), sentence)))\n",
    "    return cleaned\n",
    "\n",
    "def clean_empty_words(sentences):\n",
    "    cleaned = []\n",
    "    for sentence in sentences:\n",
    "        cleaned.append(list(filter(lambda x: x != '' and x != \"\", sentence)))\n",
    "    return cleaned\n",
    "\n",
    "def clean_single_letters(sentences):\n",
    "    cleaned = []\n",
    "    for sentence in sentences:\n",
    "        cleaned.append(list(filter(lambda x: len(x) != 1, sentence)))\n",
    "    return cleaned\n",
    "\n",
    "def clean_empty_sentences(sentences):\n",
    "    return list(filter(lambda x : len(x) != 0, sentences))\n",
    "\n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: clean_non_alphabetic_characters(x)) \n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: clean_empty_words(x))  \n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: clean_single_letters(x)) \n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: clean_empty_sentences(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store preproccesed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In further steps, it is needed to directly access the processed data. Since it is not efficient to preprocess the data with each execution, it gets saved as a new file that contains the processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.to_csv (r'./export_dataframe.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we flatten our data.\n",
    "\n",
    "BUG: wenn man es das erste mal ausfrührt muss man eine zeile im helper auskommentieren die probiert die später erstellte datei zu erstellen, rennen hier etwas in ein henne ei problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_fresh_flatted_copy\n",
    "\n",
    "transformed_opt_df = get_fresh_flatted_copy(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove lyrics that are too long\n",
    "We checked, whether or not removing songs that are too long increases our accuracy. However, it was not possible to properly separate long lyrics from \"regular\" lyrics. As such, this step did not bear great results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def identifyLongLyrics(data_df, capsize = 1500):\n",
    "    data_df['size'] = data_df['lyric'].transform(lambda x: len(x))  \n",
    "    data_df = data_df.query('size <= ' + str(capsize))\n",
    "    return data_df\n",
    "    \n",
    "    \n",
    "def identifyShortLyrics(data_df, minSize = 10):\n",
    "    data_df['size'] = data_df['lyric'].transform(lambda x: len(x))  \n",
    "    data_df = data_df.query('size >= ' + str(minSize))\n",
    "    return data_df\n",
    "\n",
    "transformed_opt_df = identifyLongLyrics(transformed_opt_df)\n",
    "transformed_opt_df = identifyShortLyrics(transformed_opt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove additional genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While analyizing our data, we observed that after removing all non-english songs, there are genres that contain almost no songs and therefore are unnecessary for the predictions. Thus, we removed all genres that contain less songs than a given threshhold (default value was 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filterSmallGenres(data, level='lyric',threshold = 100):\n",
    "    genres = data.genre.unique()\n",
    "\n",
    "    df_container = {}\n",
    "\n",
    "    for genre in genres:\n",
    "        df_container[genre] = data[data.genre.eq(genre)]\n",
    "    \n",
    "    for key in list(df_container.keys()):\n",
    "        if(len(df_container[key]) < threshold):\n",
    "            del df_container[key]\n",
    "\n",
    "    return pd.concat(df_container.values())\n",
    "\n",
    "transformed_opt_df = filterSmallGenres(transformed_opt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normilization is a common way of preprocessing data and therefore is also used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeData(data, level='lyric'):\n",
    "    genres = data.genre.unique()\n",
    "\n",
    "    df_container = {}\n",
    "\n",
    "    #Baue einen dict anhand der existierenden genres\n",
    "    for genre in genres:\n",
    "        df_container[genre] = data[data.genre.eq(genre)]\n",
    "    \n",
    "    sampleSize = 0\n",
    "    \n",
    "    #Finde heraus welches genre die wenigsten lieder enthält\n",
    "    for key in list(df_container.keys()):\n",
    "        if(sampleSize == 0):\n",
    "            sampleSize = len(df_container[key])\n",
    "        else:\n",
    "            if(len(df_container[key]) < sampleSize):\n",
    "                sampleSize = len(df_container[key])\n",
    "            \n",
    "    #Pick mir einen Sample anhand des wertes sampleSize\n",
    "    for key in list(df_container.keys()):\n",
    "        df_container[key] = df_container[key].sample(n = sampleSize);\n",
    "    \n",
    "    return pd.concat(df_container.values())\n",
    "    \n",
    "transformed_opt_df = normalizeData(transformed_opt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store optional preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optionally preprocessed data is again saved in another file for easier access. In the following steps, there is always the option to either use the general preprocessed data or to use the optional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_opt_df.to_csv (r'./export_dataframe_opt.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the data\n",
    "Before we use neural networks and machine learning algorithms zu analyze our data, we want to give a general overview of our now preprocessed data. We take a look at the genre and frequency distribution of our data as well as other information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genre distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will look at the distribution of generes in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helper import get_fresh_copy\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "sns.countplot(data=get_fresh_copy(), x='genre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to display a frequency distribution of the most common words in our dataset. We also take a look at the distribution of stopwords of different genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "def lengthDistribution(data_df):\n",
    "    genres = data_df.genre.unique()\n",
    "\n",
    "    lengthDistribution\n",
    "    df_container = {}\n",
    "    fd_container = {}\n",
    "\n",
    "    total_word_count = {}\n",
    "\n",
    "    merged_fd_df = pd.DataFrame(data = nltk.FreqDist(itertools.chain.from_iterable(data_df['lyric'])).items(), columns=['words', 'frequency'])\n",
    "    merged_fd_df = merged_fd_df.sort_values(\"frequency\", ascending=False)\n",
    "\n",
    "    for genre in genres:\n",
    "        df_container[genre] = data_df[data_df.genre.eq(genre)]\n",
    "        fd_container[genre] = nltk.FreqDist(itertools.chain.from_iterable(df_container[genre]['lyric']))\n",
    "        total_word_count[genre] = sum(fd_container[genre].values())\n",
    "        fd_container[genre] = pd.DataFrame(fd_container[genre].items(), columns=['words', genre])\n",
    "        fd_container[genre][genre] = fd_container[genre][genre].transform(lambda x: x/total_word_count[genre]) \n",
    "        merged_fd_df = merged_fd_df.merge(fd_container[genre], on='words')\n",
    "\n",
    "    merged_fd_df = merged_fd_df.drop(['frequency'], axis=1)\n",
    "    merged_fd_df = merged_fd_df.set_index('words')\n",
    "    \n",
    "    return merged_fd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import get_fresh_flatted_copy\n",
    "\n",
    "        \n",
    "length_distribution_df = lengthDistribution(get_fresh_flatted_copy(1))\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.xticks(rotation=45)    \n",
    "sns.lineplot(data=length_distribution_df.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import get_fresh_copy\n",
    "\n",
    "        \n",
    "length_distribution_df = lengthDistribution(get_fresh_copy(opt=True))\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.xticks(rotation=45)    \n",
    "sns.lineplot(data=length_distribution_df.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of lyric length by genre (word based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a look whether some genres are more likely to have longer lyrics than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def freqenzyDistribution(data, level='lyric'):\n",
    "    data[level] = data[level].transform(lambda x: len(x))\n",
    "\n",
    "    genres = data.genre.unique()\n",
    "\n",
    "    df_container = {}\n",
    "    mean_container = {}\n",
    "    std_container = {}\n",
    "    counts_container = {}\n",
    "\n",
    "    for genre in genres:\n",
    "        df_container[genre] = pickle.loads(pickle.dumps(data[data.genre.eq(genre)]))\n",
    "\n",
    "    for genre in genres:\n",
    "        mean_container[genre] = df_container[genre][level].values.mean()\n",
    "\n",
    "    for genre in genres:\n",
    "        std_container[genre] = df_container[genre][level].values.std()\n",
    "\n",
    "    for genre in genres:\n",
    "        counts_container[genre] = df_container[genre][level].value_counts(normalize=True)\n",
    "\n",
    "    for genre in genres:\n",
    "        counts_container[genre] = pd.DataFrame.from_dict(counts_container[genre])\n",
    "        counts_container[genre]['genre'] = genre\n",
    "        counts_container[genre] = counts_container[genre].reset_index()\n",
    "        counts_container[genre] = counts_container[genre].rename(columns={level: 'count'})\n",
    "        counts_container[genre] = counts_container[genre].rename(columns={'index': 'length'})\n",
    "\n",
    "    return pd.concat(counts_container.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from helper import get_fresh_flatted_copy\n",
    "    \n",
    "        \n",
    "flatted_df = get_fresh_flatted_copy(1)\n",
    "result_df = freqenzyDistribution(flatted_df)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.xticks(rotation=45)\n",
    "sns.scatterplot(data=result_df, x=\"length\", y=\"count\", hue=\"genre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional preprocessed dataset\n",
    "If we take a look at only our three main genres, Rock, Pop and Hip Hop, we see, that generally Rock songs are the shortest of all of them. Most Rock songs are less than 200 words long (after removing stopwords) while Hip Hop contains the longest songs, some of which contain more than 600 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from helper import get_fresh_flatted_copy\n",
    "    \n",
    "        \n",
    "flatted_df = get_fresh_copy(opt=True)\n",
    "result_df = freqenzyDistribution(flatted_df)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.xticks(rotation=45)\n",
    "sns.scatterplot(data=result_df, x=\"length\", y=\"count\", hue=\"genre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of lyric length by genre (verse based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take another look at the distibution of lyric length, this time, the length is based on a whole verse instead of words. Our result is that most songs consist of less than 8 verses while Hip Hop lyrics are generally longer than other genres which confirms the word based distribution in which Hip Hop also is generally longer than Rock or Pop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from helper import get_fresh_copy, melt\n",
    "\n",
    "flatted_df = get_fresh_copy(1)\n",
    "flatted_df = pd.DataFrame(melt(flatted_df))\n",
    "\n",
    "result_df = freqenzyDistribution(flatted_df, 'verse')\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.xticks(rotation=45)\n",
    "sns.lineplot(data=result_df[result_df['count']>= 0.001], x=\"length\", y=\"count\", hue=\"genre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of lyric length by genre (lyrics/verse based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we look if some generes as more likly to have shorter or longer lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from helper import get_fresh_copy\n",
    "\n",
    "flatted_df = get_fresh_copy(1)\n",
    "result_df = freqenzyDistribution(flatted_df)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.xticks(rotation=45)\n",
    "sns.lineplot(data=result_df, x=\"length\", y=\"count\", hue=\"genre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo nachschauen wieviele wörter wie oft auftauchen\n",
    "#wäre cool wenn wir das noch hinbekommen aber das formulieren der texte ist viel wichtiger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to Helper\n",
    "#https://stackoverflow.com/questions/42406233/how-to-add-title-to-seaborn-boxplot\n",
    "def print_graph(history_a, history_b):\n",
    "    history_df = pd.DataFrame(history_a.history)\n",
    "\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.xticks(rotation=45)\n",
    "    sns.lineplot(data=history_df).set_title('General Data')\n",
    "\n",
    "    history_df = pd.DataFrame(history_b.history)\n",
    "\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.xticks(rotation=45)\n",
    "    sns.lineplot(data=history_df).set_title(\"Optional Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from helper import get_fresh_flatted_copy, get_fresh_copy\n",
    "\n",
    "def prepData(data_df):\n",
    "    preprocessed_df = data_df\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(preprocessed_df['lyric'], preprocessed_df['genre'], test_size=0.20)\n",
    "    tokenizer = Tokenizer(num_words=10000)\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train = le.transform(y_train)\n",
    "    y_test = le.transform(y_test)\n",
    "\n",
    "    num_classes =  len(preprocessed_df['genre'].unique())\n",
    "    tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "    x_train = tokenizer.texts_to_matrix(x_train, mode='tfidf')\n",
    "    x_test = tokenizer.texts_to_matrix(x_test, mode='tfidf')\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train)\n",
    "    y_test = keras.utils.to_categorical(y_test)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, num_classes\n",
    "\n",
    "g_x_train, g_x_test, g_y_train, g_y_test, g_num_classes = prepData(get_fresh_flatted_copy())\n",
    "o_x_train, o_x_test, o_y_train, o_y_test, o_num_classes = prepData(get_fresh_copy(opt=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier(strategy=\"most_frequent\", random_state=10)\n",
    "dummy.fit(g_x_train, g_y_train)\n",
    "most_frequent_score = dummy.score(g_x_test, g_y_test)\n",
    "print(\"Baseline by always choosing the most frequent label: \", most_frequent_score) # always predicts most frequent label\n",
    "\n",
    "dummy = DummyClassifier(strategy=\"uniform\", random_state=10)\n",
    "dummy.fit(g_x_train, g_y_train)\n",
    "uniform_score = dummy.score(g_x_test, g_y_test)\n",
    "print(\"Baseline by always predicting label at random: \", uniform_score) # random prediction\n",
    "\n",
    "dummy = DummyClassifier(strategy=\"stratified\", random_state=10, constant=\"rock\")\n",
    "dummy.fit(g_x_train, g_y_train)\n",
    "stratified_score = dummy.score(g_x_test, g_y_test)\n",
    "print(\"Baseline by respecting the training sets class distribution: \", stratified_score) # prediction by respecting the training sets class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(10000, )))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(g_num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "history_g = model.fit(g_x_train, g_y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
    "\n",
    "history_g_dict = history_g.history\n",
    "acc_g = history_g_dict['accuracy']\n",
    "vacc_g = history_g_dict['val_accuracy']\n",
    "loss_g = history_g_dict['loss']\n",
    "vloss_g = history_g_dict['val_loss']\n",
    "\n",
    "\n",
    "score_g = model.evaluate(g_x_test, g_y_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(10000, )))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(o_num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "history_o = model.fit(o_x_train, o_y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
    "\n",
    "history_o_dict = history_o.history\n",
    "acc_o = history_o_dict['accuracy']\n",
    "vacc_o = history_o_dict['val_accuracy']\n",
    "loss_o = history_o_dict['loss']\n",
    "vloss_o = history_o_dict['val_loss']\n",
    "\n",
    "score_o = model.evaluate(o_x_test, o_y_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print_graph(history_g, history_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dense Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(10000, )))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(g_num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "history_g = model.fit(g_x_train, g_y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
    "\n",
    "history_g_dict_deep = history_g.history\n",
    "acc_g_deep = history_g_dict_deep['accuracy']\n",
    "vacc_g_deep = history_g_dict_deep['val_accuracy']\n",
    "loss_g_deep = history_g_dict_deep['loss']\n",
    "vloss_g_deep = history_g_dict_deep['val_loss']\n",
    "\n",
    "score_g_deep = model.evaluate(g_x_test, g_y_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(10000, )))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(o_num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "history_o = model.fit(o_x_train, o_y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
    "\n",
    "history_o_dict_deep = history_o.history\n",
    "acc_o_deep = history_o_dict_deep['accuracy']\n",
    "vacc_o_deep = history_o_dict_deep['val_accuracy']\n",
    "loss_o_deep = history__dict_deep['loss']\n",
    "vloss_o_deep = history_o_dict_deep['val_loss']\n",
    "\n",
    "score_o = model.evaluate(o_x_test, o_y_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print_graph(history_g, history_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Deep Dense Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2048, input_shape=(10000, )))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2048))\n",
    "model.add(Dense(g_num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "history_g = model.fit(g_x_train, g_y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
    "\n",
    "history_g_dict_bigdeep = history_g.history\n",
    "acc_g_bigdeep = history_g_dict_bigdeep['accuracy']\n",
    "vacc_g_bigdeep = history_g_dict_bigdeep['val_accuracy']\n",
    "loss_g_bigdeep = history_g_dict_bigdeep['loss']\n",
    "vloss_g_bigdeep = history_g_dict_bigdeep['val_loss']\n",
    "\n",
    "score_g = model.evaluate(g_x_test, g_y_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2048, input_shape=(10000, )))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2048))\n",
    "model.add(Dense(o_num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "history_o = model.fit(o_x_train, o_y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
    "\n",
    "history_o_dict_bigdeep = history_o.history\n",
    "acc_o_bigdeep = history_o_dict_bigdeep['accuracy']\n",
    "vacc_o_bigdeep = history_o_dict_bigdeep['val_accuracy']\n",
    "loss_o_bigdeep = history_o_dict_bigdeep['loss']\n",
    "vloss_o_bigdeep = history_o_dict_bigdeep['val_loss']\n",
    "\n",
    "score_o = model.evaluate(o_x_test, o_y_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print_graph(history_g, history_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 400\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data and factorize 'genre'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from helper import melt, get_fresh_flatted_copy\n",
    "\n",
    "preprocessed_df = pd.DataFrame(get_fresh_flatted_copy(1))\n",
    "\n",
    "#https://stackoverflow.com/questions/42320834/sklearn-changing-string-class-label-to-int\n",
    "preprocessed_df.genre = pd.factorize(preprocessed_df.genre)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(preprocessed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save needed dataframes as arrays and hot-encode the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "labels_index = preprocessed_df['genre'].unique()\n",
    "texts = np.asarray(preprocessed_df['lyric'])\n",
    "labels = np.asarray(preprocessed_df['genre'])\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert lyrics to verctors and pad the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data\n",
    "split the data into a training set and a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pre-trained embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the pre-trained glove-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import loadGloveModel\n",
    "\n",
    "embeddings_index = loadGloveModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build the embedding-matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build the embedding-lyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### not pre-trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network with MaxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "innter_layer = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "innter_layer = MaxPooling1D(5)(innter_layer)\n",
    "innter_layer = Conv1D(128, 5, activation='relu')(innter_layer)\n",
    "innter_layer = MaxPooling1D(5)(innter_layer)\n",
    "innter_layer = Conv1D(128, 5, activation='relu')(innter_layer)\n",
    "innter_layer = GlobalMaxPooling1D()(innter_layer)\n",
    "innter_layer = Dense(64, activation='relu')(innter_layer)\n",
    "innter_layer = Dropout(0.5)(innter_layer)\n",
    "innter_layer = Dense(32, activation='relu')(innter_layer)\n",
    "innter_layer = Dropout(0.5)(innter_layer)\n",
    "innter_layer = Dense(16, activation='relu')(innter_layer)\n",
    "innter_layer = Dropout(0.5)(innter_layer)\n",
    "\n",
    "output_1 = Dense(len(labels_index), activation='softmax')(innter_layer)\n",
    "\n",
    "model = Model(sequence_input, output_1)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "history_dict_convolutionalPooling = history.history\n",
    "acc_convolutionalPooling = history_dict_convolutionalPooling['acc']\n",
    "vacc_convolutionalPooling = history_dict_convolutionalPooling['val_acc']\n",
    "loss_convolutionalPooling = history_dict_convolutionalPooling['loss']\n",
    "vloss_convolutionalPooling = history_dict_convolutionalPooling['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=45)\n",
    "sns.lineplot(data=history_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Bidirectional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Bidirectional, LSTM, GRU\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                    EMBEDDING_DIM,\n",
    "                    embeddings_initializer=Constant(embedding_matrix),\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=False))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Bidirectional(GRU(16, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(8)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(6))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=20,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "history_dict_recurrentBidirectional = history.history\n",
    "acc_recurrentBidirectional = history_dict_recurrentBidirectional['acc']\n",
    "vacc_recurrentBidirectional = history_dict_recurrentBidirectional['val_acc']\n",
    "loss_recurrentBidirectional = history_dict_recurrentBidirectional['loss']\n",
    "vloss_recurrentBidirectional = history_dict_recurrentBidirectional['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=45)\n",
    "sns.lineplot(data=history_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from helper import get_fresh_flatted_copy, get_fresh_copy\n",
    "\n",
    "def prepData(data_df):\n",
    "    preprocessed_df = data_df\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(list(preprocessed_df['lyric']), list(preprocessed_df['genre']), test_size=0.80)\n",
    "    #tokenizer = Tokenizer(num_words=10000)\n",
    "\n",
    "    #le = preprocessing.LabelEncoder()\n",
    "    #le.fit(y_train)\n",
    "    #y_train = le.transform(y_train)\n",
    "    #y_test = le.transform(y_test)\n",
    "\n",
    "    num_classes =  len(preprocessed_df['genre'].unique())\n",
    "    #tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "    #x_train = tokenizer.texts_to_matrix(x_train, mode='tfidf')\n",
    "    #x_test = tokenizer.texts_to_matrix(x_test, mode='tfidf')\n",
    "\n",
    "    #y_train = keras.utils.to_categorical(y_train)\n",
    "    #y_test = keras.utils.to_categorical(y_test)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, num_classes\n",
    "\n",
    "def joinSong(songs):\n",
    "    result = []\n",
    "    for i in songs:\n",
    "        delimiter = \" \"\n",
    "        delimiter = delimiter.join(i)\n",
    "        result.append(delimiter)\n",
    "    return result\n",
    "\n",
    "x_train, x_test, y_train, y_test, num_classes = prepData(get_fresh_flatted_copy(1))\n",
    "\n",
    "x_train = joinSong(x_train)\n",
    "x_test = joinSong(x_test)\n",
    "\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "#y_train = keras.utils.to_categorical(y_train)\n",
    "#y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pythonhealthcare.org/2018/12/15/104-using-free-text-for-classification-bag-of-words/\n",
    "def create_bag_of_words(x):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 ngram_range = (1,2), \\\n",
    "                                 max_features = 10000\n",
    "                                ) \n",
    "\n",
    "    # fit_transform() does two functions: First, it fits the model\n",
    "    # and learns the vocabulary; second, it transforms our training data\n",
    "    # into feature vectors. The input to fit_transform should be a list of \n",
    "    # strings. The output is a sparse array\n",
    "    \n",
    "    train_data_features = vectorizer.fit_transform(x)\n",
    "    \n",
    "    # Convert to a NumPy array for easy of handling\n",
    "    train_data_features = train_data_features.toarray()\n",
    "    \n",
    "    # tfidf transform\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    \n",
    "    tfidf = TfidfTransformer()\n",
    "    tfidf_features = tfidf.fit_transform(train_data_features).toarray()\n",
    "\n",
    "    # Get words in the vocabulary\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "   \n",
    "    return vectorizer, vocab, train_data_features, tfidf_features, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer, vocab, train_data_features, tfidf_features, tfidf  = create_bag_of_words(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bag_dictionary = pd.DataFrame()\n",
    "bag_dictionary['ngram'] = vocab\n",
    "bag_dictionary['count'] = train_data_features[0]\n",
    "bag_dictionary['tfidf_features'] = tfidf_features[0]\n",
    "\n",
    "# Sort by raw count\n",
    "bag_dictionary.sort_values(by=['count'], ascending=False, inplace=True)\n",
    "# Show top 10\n",
    "display(bag_dictionary.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(features, label):\n",
    "    print (\"Training the logistic regression model...\")\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    ml_model = LogisticRegression(C = 100,random_state = 0, max_iter=1000)\n",
    "    ml_model.fit(features, label)\n",
    "    print ('Finished')\n",
    "    return ml_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wenn datensatz klein ist muss man max_iter hochstellen damit es convergiert\n",
    "ml_model = train_logistic_regression(tfidf_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_features = vectorizer.transform(x_test)\n",
    "# Convert to numpy array\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_tfidf_features = tfidf.fit_transform(test_data_features)\n",
    "# Convert to numpy array\n",
    "test_data_tfidf_features = test_data_tfidf_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predicted_y = ml_model.predict(test_data_tfidf_features)\n",
    "correctly_identified_y = predicted_y == y_test\n",
    "accuracy = np.mean(correctly_identified_y) * 100\n",
    "print ('Accuracy = %.0f%%' %accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base line\n",
    "\n",
    "* baselines berechnen \n",
    "* Loss or Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
