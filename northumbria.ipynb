{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Download nltk\n",
    "This work makes havy use of the nltk-framework, the download can be startet with: '#nltk.download()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for GPU\n",
    "\n",
    "*If you dont have a GPU in your maschine you can ignore these section*\n",
    "\n",
    "For better performece we recomend using your GPU, if none is detectet consider adding it to your python-enviroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpu = tf.config.experimental.list_physical_devices('GPU')\n",
    "cpu = tf.config.experimental.list_physical_devices('CPU')\n",
    "\n",
    "print(\"There is\", len(gpu), \"detected GPU\")\n",
    "print(\"There is\", len(cpu), \"detected CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "We load the required data into to a pandas dataframes and join them by a common field.\n",
    "\n",
    "for convinient use we transform all identifyers to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "song_df = pd.read_csv('./song-lyrics/lyrics-data.csv')\n",
    "author_df = pd.read_csv('./song-lyrics/artists-data.csv')\n",
    "\n",
    "song_df = song_df.rename(columns={'ALink': 'Link'})\n",
    "\n",
    "raw_df = song_df.merge(author_df, on='Link') \n",
    "raw_df.columns = raw_df.columns.str.lower()\n",
    "\n",
    "print(\"There are\", len(raw_df), \"datasets loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove unneedet data\n",
    "\n",
    "we removed all samples which arnt english or are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns_to_drop = ['sname','slink','link', 'popularity', 'genres', 'songs', 'link', 'idiom']\n",
    "\n",
    "filterd_df = raw_df[raw_df.idiom.eq('ENGLISH')]\n",
    "filterd_df = filterd_df.drop_duplicates(subset='slink', keep='first') \n",
    "filterd_df = filterd_df.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General preprocessing\n",
    "\n",
    "for further procssesing the lyrics get tokenized, lemmatized and cleand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "transformed_df = filterd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "transformed_df['lyric'] = transformed_df['lyric'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def lyric_to_verse(lyric):\n",
    "    return nltk.sent_tokenize(lyric,language='english')\n",
    "\n",
    "def verse_to_word(verse):\n",
    "    return nltk.word_tokenize(verse, language='english')\n",
    "\n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: lyric_to_verse(x)) \n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: [verse_to_word(verse) for verse in x]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatiz\n",
    "\n",
    "now we lemmatize our tokens, this may take a wile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "#by http://textmining.wp.hs-hannover.de/Preprocessing.html\n",
    "def pos_tagger(sentences):\n",
    "    return [nltk.pos_tag(sent) for sent in sentences]\n",
    "\n",
    "def wntag(pttag):\n",
    "    if pttag in ['JJ', 'JJR', 'JJS']:\n",
    "        return wordnet.ADJ\n",
    "    elif pttag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        return wordnet.NOUN\n",
    "    elif pttag in ['RB', 'RBR', 'RBS']:\n",
    "        return wordnet.ADV\n",
    "    elif pttag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        return wordnet.VERB\n",
    "    return None\n",
    "\n",
    "def lemmatize(lemmatizer,word,pos):\n",
    "    if pos == None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word,pos)\n",
    "    \n",
    "def lem(sentences):\n",
    "    transformed = []\n",
    "    for sentence in sentences:\n",
    "        transformed.append([lemmatize(lemmatizer,word,wntag(pos)) for (word,pos) in sentence])\n",
    "    return transformed \n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: pos_tagger(x)) \n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: lem(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "we remove any artefacts which surviced lematisation like empty words, single letter words and all non alphabetic characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_non_alphabetic_characters(sentences):\n",
    "    cleaned = []\n",
    "    for sentence in sentences:\n",
    "        cleaned.append(list(map(lambda x: re.sub(\"[\\W_]\", '', x), sentence)))\n",
    "    return cleaned\n",
    "\n",
    "def clean_empty_words(sentences):\n",
    "    cleaned = []\n",
    "    for sentence in sentences:\n",
    "        cleaned.append(list(filter(lambda x: x != '' and x != \"\", sentence)))\n",
    "    return cleaned\n",
    "\n",
    "def clean_single_letters(sentences):\n",
    "    cleaned = []\n",
    "    for sentence in sentences:\n",
    "        cleaned.append(list(filter(lambda x: len(x) != 1, sentence)))\n",
    "    return cleaned\n",
    "\n",
    "def clean_empty_sentences(sentences):\n",
    "    return list(filter(lambda x : len(x) != 0, sentences))\n",
    "\n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: clean_non_alphabetic_characters(x)) \n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: clean_empty_words(x))  \n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: clean_single_letters(x)) \n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: clean_empty_sentences(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove to long Lyrics\n",
    "\n",
    "checken ob lieder die länger wie 1000 oder 1500 wirklich save sind, am besten erstmal händisch nachschauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove additional Genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alle generes rauswerfen die für weniger als x lieder haben "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store preproccesd data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.to_csv (r'./export_dataframe.csv', index = False, header=True)\n",
    "transformed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load form disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "transformed_df = pd.read_csv(r'./export_dataframe.csv', converters={'lyric': eval})\n",
    "transformed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kleiner helfer um firsche dfs zu bekommen falls man destruktiv daten bearbeitet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import flatten\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "restored_df = pd.read_csv(r'./export_dataframe.csv', converters={'lyric': eval})\n",
    "\n",
    "def get_fresh_copy(frac=1):\n",
    "    copy_df = pickle.loads(pickle.dumps(restored_df.sample(frac=frac, random_state=1)))\n",
    "    return copy_df\n",
    "\n",
    "def get_fresh_flatted_copy(frac=1):\n",
    "    copy_df = get_fresh_copy(frac)\n",
    "    copy_df['lyric'] = copy_df['lyric'].transform(lambda x: flatten(x))\n",
    "    return copy_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GenreDistribution\n",
    "\n",
    "first we will look at the distribution of generes in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.countplot(data=get_fresh_copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FreqencyDistribution\n",
    "\n",
    "freqency distribution of the most common words in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to see if removing stopwords is a good idea we have a look ate the destribuition of stopwords over the difertent genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo refactor \n",
    "\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "        \n",
    "flatted_df = get_fresh_flatted_copy(1)\n",
    "genres = flatted_df.genre.unique()\n",
    "\n",
    "df_container = {}\n",
    "fd_container = {}\n",
    "\n",
    "total_word_count = {}\n",
    "\n",
    "merged_fd_df = pd.DataFrame(data = nltk.FreqDist(itertools.chain.from_iterable(flatted_df['lyric'])).items(), columns=['words', 'frequency'])\n",
    "merged_fd_df = merged_fd_df.sort_values(\"frequency\", ascending=False)\n",
    "\n",
    "#in viele kleine loops zerhacken und erklären was passiert\n",
    "#und vielleicht noch den duchschnitt hinzufügen, allso nicht nach genre gefilterd\n",
    "for genre in genres:\n",
    "    df_container[genre] = flatted_df[flatted_df.genre.eq(genre)]\n",
    "    fd_container[genre] = nltk.FreqDist(itertools.chain.from_iterable(df_container[genre]['lyric']))\n",
    "    total_word_count[genre] = sum(fd_container[genre].values())\n",
    "    fd_container[genre] = pd.DataFrame(fd_container[genre].items(), columns=['words', genre])\n",
    "    fd_container[genre][genre] = fd_container[genre][genre].transform(lambda x: x/total_word_count[genre]) \n",
    "    merged_fd_df = merged_fd_df.merge(fd_container[genre], on='words')\n",
    "\n",
    "    \n",
    "merged_fd_df = merged_fd_df.drop(['frequency'], axis=1)\n",
    "merged_fd_df = merged_fd_df.set_index('words')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=45)    \n",
    "sns.lineplot(data=merged_fd_df.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyricslength Distribution by genre(word)\n",
    "\n",
    "we look if some generes as more likly to have shorter or longer lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freqenzyDistribution(data, level='lyric'):\n",
    "    data[level] = data[level].transform(lambda x: len(x))\n",
    "\n",
    "    genres = data.genre.unique()\n",
    "\n",
    "    df_container = {}\n",
    "    mean_container = {}\n",
    "    std_container = {}\n",
    "    counts_container = {}\n",
    "\n",
    "    for genre in genres:\n",
    "        df_container[genre] = pickle.loads(pickle.dumps(data[data.genre.eq(genre)]))\n",
    "\n",
    "    for genre in genres:\n",
    "        mean_container[genre] = df_container[genre][level].values.mean()\n",
    "\n",
    "    for genre in genres:\n",
    "        std_container[genre] = df_container[genre][level].values.std()\n",
    "\n",
    "    for genre in genres:\n",
    "        counts_container[genre] = df_container[genre][level].value_counts(normalize=True)\n",
    "\n",
    "    for genre in genres:\n",
    "        counts_container[genre] = pd.DataFrame.from_dict(counts_container[genre])\n",
    "        counts_container[genre]['genre'] = genre\n",
    "        counts_container[genre] = counts_container[genre].reset_index()\n",
    "        counts_container[genre] = counts_container[genre].rename(columns={level: 'count'})\n",
    "        counts_container[genre] = counts_container[genre].rename(columns={'index': 'length'})\n",
    "\n",
    "    return pd.concat(counts_container.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "    \n",
    "        \n",
    "flatted_df = get_fresh_flatted_copy(1)\n",
    "result_df = freqenzyDistribution(flatted_df)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=45)\n",
    "sns.scatterplot(data=result_df[result_df['count']>= 0.001], x=\"length\", y=\"count\", hue=\"genre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyricslength Distribution by genre(vers)\n",
    "\n",
    "we look if some generes as more likly to have shorter or longer lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from helper import melt\n",
    "\n",
    "flatted_df = get_fresh_copy(1)\n",
    "flatted_df = pd.DataFrame(melt(flatted_df))\n",
    "\n",
    "result_df = freqenzyDistribution(flatted_df, 'verse')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=45)\n",
    "sns.lineplot(data=result_df[result_df['count']>= 0.001], x=\"length\", y=\"count\", hue=\"genre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyricslength Distribution by genre(lyric)\n",
    "\n",
    "we look if some generes as more likly to have shorter or longer lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "flatted_df = get_fresh_copy(1)\n",
    "result_df = freqenzyDistribution(flatted_df)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=45)\n",
    "sns.lineplot(data=result_df[result_df['count']>= 0.001], x=\"length\", y=\"count\", hue=\"genre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo recfactor\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import keras\n",
    "\n",
    "preprocessed_df = get_fresh_flatted_copy()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(preprocessed_df['lyric'], preprocessed_df['genre'], test_size=0.20)\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "num_classes =  len(transformed_df['genre'].unique())\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "print(num_classes)\n",
    "\n",
    "x_train = tokenizer.texts_to_matrix(x_train, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(x_test, mode='tfidf')\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(4096, input_shape=(10000, ), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1024, activation='linear'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('tf-gpu': conda)",
   "language": "python",
   "name": "python38564bittfgpuconda863f247dbe354be5ace420211fd8549e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
