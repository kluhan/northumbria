{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Download nltk\n",
    "This work makes havy use of the nltk-framework, the download can be startet with: '#nltk.download()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for GPU\n",
    "\n",
    "*If you dont have a GPU in your maschine you can ignore these section*\n",
    "\n",
    "For better performece we recomend using your GPU, if none is detectet consider adding it to your python-enviroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU')\n",
    "\n",
    "print(\"There is\", len(gpus), \"detected GPU\")\n",
    "print(\"There is\", len(cpus), \"detected CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition we recommond enlable memory growth and restrict the amount of GPU-Memory used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@starriet87/tensorflow-2-0-wanna-limit-gpu-memory-10ad474e2528\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "  try:\n",
    "    #tf_vdc = [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7168)]\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpus[0], tf_vdc)\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "We load the required data into to a pandas dataframes and join them by a common field.\n",
    "\n",
    "for convinient use we transform all identifyers to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "song_df = pd.read_csv('./song-lyrics/lyrics-data.csv')\n",
    "author_df = pd.read_csv('./song-lyrics/artists-data.csv')\n",
    "\n",
    "song_df = song_df.rename(columns={'ALink': 'Link'})\n",
    "\n",
    "raw_df = song_df.merge(author_df, on='Link') \n",
    "raw_df.columns = raw_df.columns.str.lower()\n",
    "\n",
    "print(\"There are\", len(raw_df), \"datasets loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove unneedet data\n",
    "\n",
    "we removed all samples which arnt english or are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns_to_drop = ['sname','slink','link', 'popularity', 'genres', 'songs', 'link', 'idiom']\n",
    "\n",
    "filterd_df = raw_df[raw_df.idiom.eq('ENGLISH')]\n",
    "filterd_df = filterd_df.drop_duplicates(subset='slink', keep='first') \n",
    "filterd_df = filterd_df.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General preprocessing\n",
    "\n",
    "for further procssesing the lyrics get tokenized, lemmatized and cleand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "transformed_df = filterd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "transformed_df['lyric'] = transformed_df['lyric'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def lyric_to_verse(lyric):\n",
    "    return nltk.sent_tokenize(lyric,language='english')\n",
    "\n",
    "def verse_to_word(verse):\n",
    "    return nltk.word_tokenize(verse, language='english')\n",
    "\n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: lyric_to_verse(x)) \n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: [verse_to_word(verse) for verse in x]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatiz\n",
    "\n",
    "now we lemmatize our tokens, this may take a wile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "#by http://textmining.wp.hs-hannover.de/Preprocessing.html\n",
    "def pos_tagger(sentences):\n",
    "    return [nltk.pos_tag(sent) for sent in sentences]\n",
    "\n",
    "def wntag(pttag):\n",
    "    if pttag in ['JJ', 'JJR', 'JJS']:\n",
    "        return wordnet.ADJ\n",
    "    elif pttag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        return wordnet.NOUN\n",
    "    elif pttag in ['RB', 'RBR', 'RBS']:\n",
    "        return wordnet.ADV\n",
    "    elif pttag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        return wordnet.VERB\n",
    "    return None\n",
    "\n",
    "def lemmatize(lemmatizer,word,pos):\n",
    "    if pos == None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word,pos)\n",
    "    \n",
    "def lem(sentences):\n",
    "    transformed = []\n",
    "    for sentence in sentences:\n",
    "        transformed.append([lemmatize(lemmatizer,word,wntag(pos)) for (word,pos) in sentence])\n",
    "    return transformed \n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: pos_tagger(x)) \n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: lem(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "we remove any artefacts which surviced lematisation like empty words, single letter words and all non alphabetic characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_non_alphabetic_characters(sentences):\n",
    "    cleaned = []\n",
    "    for sentence in sentences:\n",
    "        cleaned.append(list(map(lambda x: re.sub(\"[\\W_]\", '', x), sentence)))\n",
    "    return cleaned\n",
    "\n",
    "def clean_empty_words(sentences):\n",
    "    cleaned = []\n",
    "    for sentence in sentences:\n",
    "        cleaned.append(list(filter(lambda x: x != '' and x != \"\", sentence)))\n",
    "    return cleaned\n",
    "\n",
    "def clean_single_letters(sentences):\n",
    "    cleaned = []\n",
    "    for sentence in sentences:\n",
    "        cleaned.append(list(filter(lambda x: len(x) != 1, sentence)))\n",
    "    return cleaned\n",
    "\n",
    "def clean_empty_sentences(sentences):\n",
    "    return list(filter(lambda x : len(x) != 0, sentences))\n",
    "\n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: clean_non_alphabetic_characters(x)) \n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: clean_empty_words(x))  \n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: clean_single_letters(x)) \n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: clean_empty_sentences(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store preproccesd data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.to_csv (r'./export_dataframe.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove to long Lyrics\n",
    "\n",
    "checken ob lieder die länger wie 1000 oder 1500 wirklich save sind, am besten erstmal händisch nachschauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "799"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helper import get_fresh_flatted_copy\n",
    "\n",
    "flatted_df = get_fresh_flatted_copy(1)\n",
    "def identifyLongLyrics(data_df, capsize = 1500):\n",
    "    data_df['size'] = data_df['lyric'].transform(lambda x: len(x))  \n",
    "    data_df = data_df.query('size >= ' + str(capsize))\n",
    "    display(len(data_df))\n",
    "    data_df = data_df.sample(n = 10)\n",
    "    \n",
    "def identifyShortLyrics(data_df, minSize = 40):\n",
    "    data_df['size'] = data_df['lyric'].transform(lambda x: len(x))  \n",
    "    data_df = data_df.query('size <= ' + str(minSize))\n",
    "    display(len(data_df))\n",
    "    data_df = data_df.sample(n = 10)\n",
    "\n",
    "identifyLongLyrics(flatted_df)\n",
    "identifyShortLyrics(flatted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove additional Genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alle generes rauswerfen die für weniger als x lieder haben "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_fresh_flatted_copy\n",
    "import pandas as pd\n",
    "\n",
    "flatted_df = get_fresh_flatted_copy(1)\n",
    "\n",
    "def filterSmallGenres(data, level='lyric',threshold = 100):\n",
    "    genres = data.genre.unique()\n",
    "\n",
    "    df_container = {}\n",
    "\n",
    "    for genre in genres:\n",
    "        df_container[genre] = data[data.genre.eq(genre)]\n",
    "    \n",
    "    for key in list(df_container.keys()):\n",
    "        if(len(df_container[key]) < threshold):\n",
    "            del df_container[key]\n",
    "\n",
    "    return pd.concat(df_container.values())\n",
    "    \n",
    "transformed_opt_df = filterSmallGenres(flatted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize dataset\n",
    "\n",
    "schau das du von allen genres gleich viele lieder hast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalizeData(data, level='lyric'):\n",
    "    genres = data.genre.unique()\n",
    "\n",
    "    df_container = {}\n",
    "\n",
    "    #Baue einen dict anhand der existierenden genres\n",
    "    for genre in genres:\n",
    "        df_container[genre] = data[data.genre.eq(genre)]\n",
    "    \n",
    "    sampleSize = 0\n",
    "    \n",
    "    #Finde heraus welches genre die wenigsten lieder enthält\n",
    "    for key in list(df_container.keys()):\n",
    "        if(sampleSize == 0):\n",
    "            sampleSize = len(df_container[key])\n",
    "        else:\n",
    "            if(len(df_container[key]) < sampleSize):\n",
    "                sampleSize = len(df_container[key])\n",
    "            \n",
    "    #Pick mir einen Sample anhand des wertes sampleSize\n",
    "    for key in list(df_container.keys()):\n",
    "        df_container[key] = df_container[key].sample(n = sampleSize);\n",
    "    \n",
    "    return pd.concat(df_container.values())\n",
    "    \n",
    "transformed_opt_df = normalizeData(transformed_opt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store optional Preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_opt_df.to_csv (r'./export_dataframe_opt.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GenreDistribution\n",
    "\n",
    "first we will look at the distribution of generes in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import get_fresh_copy\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.countplot(data=get_fresh_copy(), x='genre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FreqencyDistribution\n",
    "\n",
    "freqency distribution of the most common words in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to see if removing stopwords is a good idea we have a look ate the destribuition of stopwords over the difertent genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo nachschauen wieviele wörter wie oft auftauchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo refactor \n",
    "\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from helper import get_fresh_flatted_copy\n",
    "        \n",
    "flatted_df = get_fresh_flatted_copy(1)\n",
    "genres = flatted_df.genre.unique()\n",
    "\n",
    "df_container = {}\n",
    "fd_container = {}\n",
    "\n",
    "total_word_count = {}\n",
    "\n",
    "merged_fd_df = pd.DataFrame(data = nltk.FreqDist(itertools.chain.from_iterable(flatted_df['lyric'])).items(), columns=['words', 'frequency'])\n",
    "merged_fd_df = merged_fd_df.sort_values(\"frequency\", ascending=False)\n",
    "\n",
    "#in viele kleine loops zerhacken und erklären was passiert\n",
    "#und vielleicht noch den duchschnitt hinzufügen, allso nicht nach genre gefilterd\n",
    "for genre in genres:\n",
    "    df_container[genre] = flatted_df[flatted_df.genre.eq(genre)]\n",
    "    fd_container[genre] = nltk.FreqDist(itertools.chain.from_iterable(df_container[genre]['lyric']))\n",
    "    total_word_count[genre] = sum(fd_container[genre].values())\n",
    "    fd_container[genre] = pd.DataFrame(fd_container[genre].items(), columns=['words', genre])\n",
    "    fd_container[genre][genre] = fd_container[genre][genre].transform(lambda x: x/total_word_count[genre]) \n",
    "    merged_fd_df = merged_fd_df.merge(fd_container[genre], on='words')\n",
    "\n",
    "    \n",
    "merged_fd_df = merged_fd_df.drop(['frequency'], axis=1)\n",
    "merged_fd_df = merged_fd_df.set_index('words')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=45)    \n",
    "sns.lineplot(data=merged_fd_df.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyricslength Distribution by genre(word)\n",
    "\n",
    "we look if some generes as more likly to have shorter or longer lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freqenzyDistribution(data, level='lyric'):\n",
    "    data[level] = data[level].transform(lambda x: len(x))\n",
    "\n",
    "    genres = data.genre.unique()\n",
    "\n",
    "    df_container = {}\n",
    "    mean_container = {}\n",
    "    std_container = {}\n",
    "    counts_container = {}\n",
    "\n",
    "    for genre in genres:\n",
    "        df_container[genre] = pickle.loads(pickle.dumps(data[data.genre.eq(genre)]))\n",
    "\n",
    "    for genre in genres:\n",
    "        mean_container[genre] = df_container[genre][level].values.mean()\n",
    "\n",
    "    for genre in genres:\n",
    "        std_container[genre] = df_container[genre][level].values.std()\n",
    "\n",
    "    for genre in genres:\n",
    "        counts_container[genre] = df_container[genre][level].value_counts(normalize=True)\n",
    "\n",
    "    for genre in genres:\n",
    "        counts_container[genre] = pd.DataFrame.from_dict(counts_container[genre])\n",
    "        counts_container[genre]['genre'] = genre\n",
    "        counts_container[genre] = counts_container[genre].reset_index()\n",
    "        counts_container[genre] = counts_container[genre].rename(columns={level: 'count'})\n",
    "        counts_container[genre] = counts_container[genre].rename(columns={'index': 'length'})\n",
    "\n",
    "    return pd.concat(counts_container.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from helper import get_fresh_flatted_copy\n",
    "    \n",
    "        \n",
    "flatted_df = get_fresh_flatted_copy(1)\n",
    "result_df = freqenzyDistribution(flatted_df)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=45)\n",
    "sns.scatterplot(data=result_df, x=\"length\", y=\"count\", hue=\"genre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyricslength Distribution by genre(vers)\n",
    "\n",
    "we look if some generes as more likly to have shorter or longer lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from helper import melt\n",
    "from helper import get_fresh_copy\n",
    "\n",
    "flatted_df = get_fresh_copy(1)\n",
    "flatted_df = pd.DataFrame(melt(flatted_df))\n",
    "\n",
    "result_df = freqenzyDistribution(flatted_df, 'verse')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=45)\n",
    "sns.lineplot(data=result_df[result_df['count']>= 0.001], x=\"length\", y=\"count\", hue=\"genre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyricslength Distribution by genre(lyric)\n",
    "\n",
    "we look if some generes as more likly to have shorter or longer lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from helper import get_fresh_copy\n",
    "\n",
    "flatted_df = get_fresh_copy(1)\n",
    "result_df = freqenzyDistribution(flatted_df)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=45)\n",
    "sns.lineplot(data=result_df[result_df['count']>= 0.001], x=\"length\", y=\"count\", hue=\"genre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple multible dense-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from helper import get_fresh_flatted_copy\n",
    "\n",
    "preprocessed_df = get_fresh_flatted_copy(0.1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(preprocessed_df['lyric'], preprocessed_df['genre'], test_size=0.4)\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "y_train = label_encoder.transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "num_classes =  len(preprocessed_df['genre'].unique())\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train = tokenizer.texts_to_matrix(x_train, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(x_test, mode='tfidf')\n",
    "\n",
    "\n",
    "display(x_train)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(10000, ), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 150\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.2)\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=45)\n",
    "sns.lineplot(data=history_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''This script loads pre-trained word embeddings (GloVe embeddings)\n",
    "into a frozen Keras Embedding layer, and uses it to\n",
    "train a text classification model on the 20 Newsgroup dataset\n",
    "(classification of newsgroup messages into 20 different categories).\n",
    "GloVe embedding data can be found at:\n",
    "http://nlp.stanford.edu/data/glove.6B.zip\n",
    "(source page: http://nlp.stanford.edu/projects/glove/)\n",
    "20 Newsgroup data can be found at:\n",
    "http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from helper import get_fresh_flatted_copy\n",
    "\n",
    "def genre_to_number(genre):\n",
    "    if(genre == 'Rock'):\n",
    "        return 0\n",
    "    if(genre == 'Hip Hop'):\n",
    "        return 1\n",
    "    if(genre == 'Pop'):\n",
    "        return 2\n",
    "    else :\n",
    "        return 3\n",
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR)\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n",
    "MAX_SEQUENCE_LENGTH = 400\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "preprocessed_df = get_fresh_flatted_copy(1)\n",
    "preprocessed_df['genre'] = preprocessed_df['genre'].transform(lambda x: genre_to_number(x)) \n",
    "\n",
    "texts = np.asarray(preprocessed_df['lyric'])\n",
    "labels = np.asarray(preprocessed_df['genre'])\n",
    "labels_index = [0, 1, 2, 3]  # dictionary mapping label name to numeric id\n",
    "                \n",
    "print('Found %s texts.' % len(texts))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "#embedding_layer = Embedding(num_words,\n",
    "#                            EMBEDDING_DIM,\n",
    "#                            embeddings_initializer=Constant(embedding_matrix),\n",
    "#                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "#                            trainable=False)\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Training model.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=45)\n",
    "sns.lineplot(data=history_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
