{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Download nltk\n",
    "This work makes havy use of the nltk-framework, the download can be startet with: '#nltk.download()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for GPU\n",
    "\n",
    "*If you dont have a GPU in your maschine you can ignore these section*\n",
    "\n",
    "For better performece we recomend using your GPU, if none is detectet consider adding it to your python-enviroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU')\n",
    "\n",
    "print(\"There is\", len(gpus), \"detected GPU\")\n",
    "print(\"There is\", len(cpus), \"detected CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition we recommond enlable memory growth and restrict the amount of GPU-Memory used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@starriet87/tensorflow-2-0-wanna-limit-gpu-memory-10ad474e2528\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "  try:\n",
    "    #tf_vdc = [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7168)]\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpus[0], tf_vdc)\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "We load the required data into to a pandas dataframes and join them by a common field.\n",
    "\n",
    "for convinient use we transform all identifyers to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "song_df = pd.read_csv('./song-lyrics/lyrics-data.csv')\n",
    "author_df = pd.read_csv('./song-lyrics/artists-data.csv')\n",
    "\n",
    "song_df = song_df.rename(columns={'ALink': 'Link'})\n",
    "\n",
    "raw_df = song_df.merge(author_df, on='Link') \n",
    "raw_df.columns = raw_df.columns.str.lower()\n",
    "\n",
    "print(\"There are\", len(raw_df), \"datasets loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove unneedet data\n",
    "\n",
    "we removed all samples which arnt english or are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns_to_drop = ['sname','slink','link', 'popularity', 'genres', 'songs', 'link', 'idiom']\n",
    "\n",
    "filterd_df = raw_df[raw_df.idiom.eq('ENGLISH')]\n",
    "filterd_df = filterd_df.drop_duplicates(subset='slink', keep='first') \n",
    "filterd_df = filterd_df.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General preprocessing\n",
    "\n",
    "for further procssesing the lyrics get tokenized, lemmatized and cleand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "transformed_df = filterd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "transformed_df['lyric'] = transformed_df['lyric'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def lyric_to_verse(lyric):\n",
    "    return nltk.sent_tokenize(lyric,language='english')\n",
    "\n",
    "def verse_to_word(verse):\n",
    "    return nltk.word_tokenize(verse, language='english')\n",
    "\n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: lyric_to_verse(x)) \n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: [verse_to_word(verse) for verse in x]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatiz\n",
    "\n",
    "now we lemmatize our tokens, this may take a wile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "#by http://textmining.wp.hs-hannover.de/Preprocessing.html\n",
    "def pos_tagger(sentences):\n",
    "    return [nltk.pos_tag(sent) for sent in sentences]\n",
    "\n",
    "def wntag(pttag):\n",
    "    if pttag in ['JJ', 'JJR', 'JJS']:\n",
    "        return wordnet.ADJ\n",
    "    elif pttag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        return wordnet.NOUN\n",
    "    elif pttag in ['RB', 'RBR', 'RBS']:\n",
    "        return wordnet.ADV\n",
    "    elif pttag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        return wordnet.VERB\n",
    "    return None\n",
    "\n",
    "def lemmatize(lemmatizer,word,pos):\n",
    "    if pos == None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word,pos)\n",
    "    \n",
    "def lem(sentences):\n",
    "    transformed = []\n",
    "    for sentence in sentences:\n",
    "        transformed.append([lemmatize(lemmatizer,word,wntag(pos)) for (word,pos) in sentence])\n",
    "    return transformed \n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: pos_tagger(x)) \n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: lem(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "we remove any artefacts which surviced lematisation like empty words, single letter words and all non alphabetic characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_non_alphabetic_characters(sentences):\n",
    "    cleaned = []\n",
    "    for sentence in sentences:\n",
    "        cleaned.append(list(map(lambda x: re.sub(\"[\\W_]\", '', x), sentence)))\n",
    "    return cleaned\n",
    "\n",
    "def clean_empty_words(sentences):\n",
    "    cleaned = []\n",
    "    for sentence in sentences:\n",
    "        cleaned.append(list(filter(lambda x: x != '' and x != \"\", sentence)))\n",
    "    return cleaned\n",
    "\n",
    "def clean_single_letters(sentences):\n",
    "    cleaned = []\n",
    "    for sentence in sentences:\n",
    "        cleaned.append(list(filter(lambda x: len(x) != 1, sentence)))\n",
    "    return cleaned\n",
    "\n",
    "def clean_empty_sentences(sentences):\n",
    "    return list(filter(lambda x : len(x) != 0, sentences))\n",
    "\n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: clean_non_alphabetic_characters(x)) \n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: clean_empty_words(x))  \n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: clean_single_letters(x)) \n",
    "transformed_df['lyric'] = transformed_df['lyric'].transform(lambda x: clean_empty_sentences(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove to long Lyrics\n",
    "\n",
    "checken ob lieder die länger wie 1000 oder 1500 wirklich save sind, am besten erstmal händisch nachschauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove additional Genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alle generes rauswerfen die für weniger als x lieder haben "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize dataset\n",
    "\n",
    "schau das du von allen genres gleich viele lieder hast "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store preproccesd data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.to_csv (r'./export_dataframe.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load form disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "transformed_df = pd.read_csv(r'./export_dataframe.csv', converters={'lyric': eval})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kleiner helfer um firsche dfs zu bekommen falls man destruktiv daten bearbeitet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import flatten\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "restored_df = pd.read_csv(r'./export_dataframe.csv', converters={'lyric': eval})\n",
    "\n",
    "def get_fresh_copy(frac=1):\n",
    "    copy_df = pickle.loads(pickle.dumps(restored_df.sample(frac=frac, random_state=1)))\n",
    "    return copy_df\n",
    "\n",
    "def get_fresh_flatted_copy(frac=1):\n",
    "    copy_df = get_fresh_copy(frac)\n",
    "    copy_df['lyric'] = copy_df['lyric'].transform(lambda x: flatten(x))\n",
    "    return copy_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GenreDistribution\n",
    "\n",
    "first we will look at the distribution of generes in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.countplot(data=get_fresh_copy(), x='genre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FreqencyDistribution\n",
    "\n",
    "freqency distribution of the most common words in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to see if removing stopwords is a good idea we have a look ate the destribuition of stopwords over the difertent genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo nachschauen wieviele wörter wie oft auftauchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo refactor \n",
    "\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "        \n",
    "flatted_df = get_fresh_flatted_copy(1)\n",
    "genres = flatted_df.genre.unique()\n",
    "\n",
    "df_container = {}\n",
    "fd_container = {}\n",
    "\n",
    "total_word_count = {}\n",
    "\n",
    "merged_fd_df = pd.DataFrame(data = nltk.FreqDist(itertools.chain.from_iterable(flatted_df['lyric'])).items(), columns=['words', 'frequency'])\n",
    "merged_fd_df = merged_fd_df.sort_values(\"frequency\", ascending=False)\n",
    "\n",
    "#in viele kleine loops zerhacken und erklären was passiert\n",
    "#und vielleicht noch den duchschnitt hinzufügen, allso nicht nach genre gefilterd\n",
    "for genre in genres:\n",
    "    df_container[genre] = flatted_df[flatted_df.genre.eq(genre)]\n",
    "    fd_container[genre] = nltk.FreqDist(itertools.chain.from_iterable(df_container[genre]['lyric']))\n",
    "    total_word_count[genre] = sum(fd_container[genre].values())\n",
    "    fd_container[genre] = pd.DataFrame(fd_container[genre].items(), columns=['words', genre])\n",
    "    fd_container[genre][genre] = fd_container[genre][genre].transform(lambda x: x/total_word_count[genre]) \n",
    "    merged_fd_df = merged_fd_df.merge(fd_container[genre], on='words')\n",
    "\n",
    "    \n",
    "merged_fd_df = merged_fd_df.drop(['frequency'], axis=1)\n",
    "merged_fd_df = merged_fd_df.set_index('words')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=45)    \n",
    "sns.lineplot(data=merged_fd_df.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyricslength Distribution by genre(word)\n",
    "\n",
    "we look if some generes as more likly to have shorter or longer lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freqenzyDistribution(data, level='lyric'):\n",
    "    data[level] = data[level].transform(lambda x: len(x))\n",
    "\n",
    "    genres = data.genre.unique()\n",
    "\n",
    "    df_container = {}\n",
    "    mean_container = {}\n",
    "    std_container = {}\n",
    "    counts_container = {}\n",
    "\n",
    "    for genre in genres:\n",
    "        df_container[genre] = pickle.loads(pickle.dumps(data[data.genre.eq(genre)]))\n",
    "\n",
    "    for genre in genres:\n",
    "        mean_container[genre] = df_container[genre][level].values.mean()\n",
    "\n",
    "    for genre in genres:\n",
    "        std_container[genre] = df_container[genre][level].values.std()\n",
    "\n",
    "    for genre in genres:\n",
    "        counts_container[genre] = df_container[genre][level].value_counts(normalize=True)\n",
    "\n",
    "    for genre in genres:\n",
    "        counts_container[genre] = pd.DataFrame.from_dict(counts_container[genre])\n",
    "        counts_container[genre]['genre'] = genre\n",
    "        counts_container[genre] = counts_container[genre].reset_index()\n",
    "        counts_container[genre] = counts_container[genre].rename(columns={level: 'count'})\n",
    "        counts_container[genre] = counts_container[genre].rename(columns={'index': 'length'})\n",
    "\n",
    "    return pd.concat(counts_container.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "    \n",
    "        \n",
    "flatted_df = get_fresh_flatted_copy(1)\n",
    "result_df = freqenzyDistribution(flatted_df)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=45)\n",
    "sns.scatterplot(data=result_df, x=\"length\", y=\"count\", hue=\"genre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyricslength Distribution by genre(vers)\n",
    "\n",
    "we look if some generes as more likly to have shorter or longer lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from helper import melt\n",
    "\n",
    "flatted_df = get_fresh_copy(1)\n",
    "flatted_df = pd.DataFrame(melt(flatted_df))\n",
    "\n",
    "result_df = freqenzyDistribution(flatted_df, 'verse')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=45)\n",
    "sns.lineplot(data=result_df[result_df['count']>= 0.001], x=\"length\", y=\"count\", hue=\"genre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyricslength Distribution by genre(lyric)\n",
    "\n",
    "we look if some generes as more likly to have shorter or longer lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "flatted_df = get_fresh_copy(1)\n",
    "result_df = freqenzyDistribution(flatted_df)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=45)\n",
    "sns.lineplot(data=result_df[result_df['count']>= 0.001], x=\"length\", y=\"count\", hue=\"genre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple multible dense-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "preprocessed_df = get_fresh_flatted_copy(0.1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(preprocessed_df['lyric'], preprocessed_df['genre'], test_size=0.4)\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "y_train = label_encoder.transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "num_classes =  len(preprocessed_df['genre'].unique())\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train = tokenizer.texts_to_matrix(x_train, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(x_test, mode='tfidf')\n",
    "\n",
    "\n",
    "display(x_train)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "display(y_test)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(10000, ), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 150\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.2)\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=45)\n",
    "sns.lineplot(data=history_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution with dropout\n",
    "https://keras.io/api/layers/core_layers/dense/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 400\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data and factorize 'genre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from helper import melt\n",
    "\n",
    "\n",
    "preprocessed_df = pd.DataFrame(get_fresh_flatted_copy(1))\n",
    "\n",
    "#https://stackoverflow.com/questions/42320834/sklearn-changing-string-class-label-to-int\n",
    "preprocessed_df.genre = pd.factorize(preprocessed_df.genre)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(preprocessed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save needed dataframes as arrays and hot-encode the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "labels_index = preprocessed_df['genre'].unique()\n",
    "texts = np.asarray(preprocessed_df['lyric'])\n",
    "labels = np.asarray(preprocessed_df['genre'])\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert lyrics to verctors and pad the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "split the data into a training set and a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pre-trained embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the pre-trained glove-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import loadGloveModel\n",
    "\n",
    "embeddings_index = loadGloveModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build the embedding-matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build the embedding-lyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### not pre-trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "innter_layer = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "innter_layer = MaxPooling1D(5)(innter_layer)\n",
    "innter_layer = Conv1D(128, 5, activation='relu')(innter_layer)\n",
    "innter_layer = MaxPooling1D(5)(innter_layer)\n",
    "innter_layer = Conv1D(128, 5, activation='relu')(innter_layer)\n",
    "innter_layer = GlobalMaxPooling1D()(innter_layer)\n",
    "innter_layer = Dense(64, activation='relu')(innter_layer)\n",
    "innter_layer = Dropout(0.5)(innter_layer)\n",
    "innter_layer = Dense(32, activation='relu')(innter_layer)\n",
    "innter_layer = Dropout(0.5)(innter_layer)\n",
    "innter_layer = Dense(16, activation='relu')(innter_layer)\n",
    "innter_layer = Dropout(0.5)(innter_layer)\n",
    "\n",
    "output_1 = Dense(len(labels_index), activation='softmax')(innter_layer)\n",
    "\n",
    "model = Model(sequence_input, output_1)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=45)\n",
    "sns.lineplot(data=history_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('tf-gpu': conda)",
   "language": "python",
   "name": "python38564bittfgpuconda863f247dbe354be5ace420211fd8549e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
